{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTn-rkUvW9_c"
      },
      "outputs": [],
      "source": [
        "# NLP - Text Parsing, Stemming, Stopword removal, Term Frequency Matrix\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import nltk.corpus\n",
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "ChatGPT is a generative artificial intelligence chatbot developed by OpenAI and launched in 2022. It is currently based on the GPT-4o large language model (LLM). ChatGPT can generate human-like conversational responses and enables users to refine and steer a conversation towards a desired length, format, style, level of detail, and language.[2] It is credited with accelerating the AI boom, which has led to ongoing rapid investment in and public attention to the field of artificial intelligence (AI).[3] Some observers have raised concern about the potential of ChatGPT and similar programs to displace human intelligence, enable plagiarism, or fuel misinformation.[4][5]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5J3dvoKiXX6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_clean = re.sub('[^a-zA-Z0-9 \\n\\.]', '', text)\n",
        "print(text_clean)"
      ],
      "metadata": {
        "id": "QDFlZysvXnNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------------\n",
        "# Cleaning text\n",
        "#----------------------------------------------------\n",
        "\n",
        "# a) to remove unnecessary spaces, punctuation and numbers\n",
        "\n",
        "# remove unnecessary spaces\n",
        "text_cleaner = re.sub(' +', ' ', text_clean)\n",
        "\n",
        "# remove unnecessary punctuation - already done above using regex, you may try to define punctuation manually\n",
        "re.sub(r'[^\\w\\s]','', text)\n",
        "\n",
        "# remove unnecessary numbers\n",
        "text_cleaner = re.sub('\\d', '', text_cleaner)\n",
        "print(text_cleaner)\n"
      ],
      "metadata": {
        "id": "FmvDGThSX6jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b) change letters to lower case\n",
        "\n",
        "# change to lowercase\n",
        "print(text_cleaner.lower())"
      ],
      "metadata": {
        "id": "ZNK8DYpBYPnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------------\n",
        "# Stopword removal\n",
        "#----------------------------------------------------\n",
        "# In the case of \"stopwords\" in the package tm\n",
        "# supported languages are: Danish, Dutch,\n",
        "# English, Finnish, French, German, Hungarian, Italian,\n",
        "# Norwegian, Portuguese, Russian, Spanish and Swedish.\n",
        "# Language names are case-sensitive.\n",
        "# remove English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(text_cleaner)\n",
        "\n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "id": "XDfe2grQYZpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if necessaary: remove your own stopwords - as a vector of words:\n",
        "stop_words_lst = ['a']\n",
        "\n",
        "for w in stop_words_lst:\n",
        "    pattern = r'\\b'+w+r'\\b'\n",
        "    filtered_text = re.sub(pattern, '', text_cleaner)\n",
        "    print (filtered_text)"
      ],
      "metadata": {
        "id": "W4946wJ9Y7TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------------\n",
        "# Stemming\n",
        "#----------------------------------------------------\n",
        "\n",
        "# Stemming reduces words to their root form\n",
        "# For example, the reduction of words \"move\", \"moved\"\n",
        "# and \"movement\" to the core \"move\".\n",
        "\n",
        "\n",
        "# stem document\n",
        "ps = PorterStemmer()\n",
        "\n",
        "words = word_tokenize(text_cleaner)\n",
        "\n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))"
      ],
      "metadata": {
        "id": "XVIjyH2VZv3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------------\n",
        "# Term frequency matrix\n",
        "#----------------------------------------------------\n",
        "\n",
        "wordlist = text_cleaner.split()\n",
        "\n",
        "wordfreq = []\n",
        "for w in wordlist:\n",
        "    wordfreq.append(wordlist.count(w))\n",
        "\n",
        "print(\"String\\n\" + text_cleaner +\"\\n\")\n",
        "print(\"List\\n\" + str(wordlist) + \"\\n\")\n",
        "print(\"Frequencies\\n\" + str(wordfreq) + \"\\n\")\n",
        "print(\"Pairs\\n\" + str(list(zip(wordlist, wordfreq))))"
      ],
      "metadata": {
        "id": "xm4fMnnoaIY3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}